{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Philip Carr\n",
    "# CS/CNS/EE_156a_Homework_7_Code_Part_3 (Jupyter Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for The PLA vs. SVM comparisons (Problems 8 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rn\n",
    "import numpy as np\n",
    "\n",
    "# Package imported for SVM: Scikit-Learn sklearn.svm SVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(x):\n",
    "    '''\n",
    "    Return the sign of a number (1 if positive, 0 if 0, or -1 if\n",
    "    negative).\n",
    "    \n",
    "    Return type: int\n",
    "    '''\n",
    "    if x >= 0:\n",
    "        return 1\n",
    "    elif x == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_line_value(x_point, y_point, m, b):\n",
    "    '''\n",
    "    Return the evaluation of the target function (line with slope m\n",
    "    and y-intercept b) for a given point (1, x_point, y_point) and\n",
    "    sign_orientation: determines whether points have value of 1 or -1\n",
    "    when above the target function line with this value being either\n",
    "    1 or -1 respectively.\n",
    "    \n",
    "    Return type: int\n",
    "    '''\n",
    "    y_line = m * x_point + b\n",
    "    if y_point >= y_line:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_function_line():\n",
    "    '''\n",
    "    Return a randomly generated target function line's slope (m) and\n",
    "    y-intercept (b).\n",
    "    \n",
    "    Return type: tuple of floats\n",
    "    '''\n",
    "    x1 = rn.uniform(-1, 1)\n",
    "    y1 = rn.uniform(-1, 1)\n",
    "    x2 = rn.uniform(-1, 1)\n",
    "    y2 = rn.uniform(-1, 1)\n",
    "    m = (y2 - y1) / (x2 - x1)\n",
    "    b = y1 - m * x1\n",
    "    return m, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_function():\n",
    "    '''\n",
    "    Return a randomly generated target function line.\n",
    "    \n",
    "    Return type: function that takes two floats and returns an int\n",
    "    (instance of the target_function_line_value function).\n",
    "    '''\n",
    "    m, b = get_target_function_line()\n",
    "    return m, b, lambda x_point, y_point: \\\n",
    "        target_function_line_value(x_point, y_point, m, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_points(N):\n",
    "    '''\n",
    "    Return a list of randomly generated points within\n",
    "    the region [-1, 1] x [-1, 1].\n",
    "    \n",
    "    Return type: list of points (each point is a list)\n",
    "    '''\n",
    "    random_points = []\n",
    "    for n in range(N):\n",
    "        x0 = 1.0 # artificial coordinate\n",
    "        x1 = rn.uniform(-1, 1)\n",
    "        x2 = rn.uniform(-1, 1)\n",
    "        random_points.append([x0, x1, x2])\n",
    "    return np.array(random_points, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_point_values(points, target_function):\n",
    "    '''\n",
    "    Return the values corresponding to the (list of) points given.\n",
    "    The returned values are function evaluations of the target\n",
    "    function target_function of the given points.\n",
    "    \n",
    "    Return type: list of ints\n",
    "    '''\n",
    "    point_values = []\n",
    "    for i in range(len(points)):\n",
    "        value = target_function(points[i,1], points[i,2])\n",
    "        point_values.append(value)\n",
    "    return np.array(point_values, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLA:\n",
    "    '''\n",
    "    This class represents the PLA (Perceptron Learning Algorithm).\n",
    "    This class contains the weights, as well as methods for running\n",
    "    the Perceptron Learning Algorithm.\n",
    "    '''\n",
    "    def __init__(self, n=2):\n",
    "        '''\n",
    "        Initialize the weights of the PLA using the given\n",
    "        dimension n of the points to work with in R^n space.\n",
    "        \n",
    "        Return type: class (PLA)\n",
    "        '''\n",
    "        self.weights = np.array([0] * (n + 1), dtype=np.float64)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        '''\n",
    "        Print the weights of the PLA.\n",
    "        \n",
    "        Return type: string\n",
    "        '''\n",
    "        print(\"LogReg weights:\", self.weights)\n",
    "    \n",
    "    def get_weights(self):\n",
    "        '''\n",
    "        Return the weights of the PLA.\n",
    "        \n",
    "        Return type: array of floats\n",
    "        '''\n",
    "        return self.weights\n",
    "    \n",
    "    def evaluate(self, point):\n",
    "        \"\"\"\n",
    "        Return a point's value using the PLA's weights.\n",
    "        \"\"\"\n",
    "        real_value = 0\n",
    "        assert(len(self.weights) == len(point))\n",
    "        for i in range(len(self.weights)):\n",
    "            real_value += self.weights[i] * point[i]\n",
    "            \n",
    "        return sign(real_value)\n",
    "    \n",
    "    def get_misclassified_indices(self, points, values):\n",
    "        '''\n",
    "        Return the list of indices of the points list corresponding\n",
    "        to the points that the PLA misclassifies (when the target\n",
    "        function and PLA hypothesis function evaluations of a point\n",
    "        disagree).\n",
    "        \n",
    "        Return type: None\n",
    "        '''\n",
    "        missclassified_indices = []\n",
    "        for i in range(len(points)):\n",
    "            if self.evaluate(points[i]) != values[i]:\n",
    "                missclassified_indices.append(i)\n",
    "        \n",
    "        return missclassified_indices\n",
    "    \n",
    "    def get_classification_error(self, points, values):\n",
    "        \"\"\"\n",
    "        Return the classification error given by\n",
    "        P[f(x) != g_PLA(x)], where f(x) is the target\n",
    "        function, g_PLA is the hypothesis function of\n",
    "        the PLA, and P[f(x) != g_PLA(x)] measures the\n",
    "        probability of disagreement between f and g_PLA\n",
    "        (classification error).\n",
    "        \"\"\"\n",
    "        misclassified_indices = \\\n",
    "            self.get_misclassified_indices(points, values)\n",
    "        return np.float64(len(misclassified_indices)) / \\\n",
    "               len(points)\n",
    "    \n",
    "    def fit(self, points, values, max_iterations=100):\n",
    "        '''\n",
    "        Return the number of iterations it takes to converge the\n",
    "        PLA to correctly classify all the given points.\n",
    "        \n",
    "        Return type: int\n",
    "        '''\n",
    "        iteration_count = 0\n",
    "        \n",
    "        # Create the list of misclassified point indices.\n",
    "        misclassified_indices = \\\n",
    "            self.get_misclassified_indices(points, values)\n",
    "        \n",
    "        while misclassified_indices != [] and \\\n",
    "              iteration_count < max_iterations:\n",
    "            \n",
    "            # Randomly choose an index of a misclassified point.\n",
    "            misclassified_index = rn.choice(misclassified_indices)\n",
    "            \n",
    "            # Iterate the weights using the chosen misclassified.\n",
    "            # point\n",
    "            self.weights += values[misclassified_index] \\\n",
    "                            * points[misclassified_index]\n",
    "            \n",
    "            # Update the list of misclassified point indices.\n",
    "            misclassified_indices = \\\n",
    "                self.get_misclassified_indices(points, values)\n",
    "            \n",
    "            iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLA_and_SVC_results(N_train):\n",
    "    \"\"\"\n",
    "    Return out-of-sample errors of the PLA (Perceptron Learning\n",
    "    Algorithm) and SVM (Support Vector Machine) and number of\n",
    "    support vectors of the SVM that resulted after being trained on\n",
    "    a randomly generated dataset of size N_train.\n",
    "    \"\"\"\n",
    "    # Initialize training data.\n",
    "    training_values = np.zeros(N_train)\n",
    "    training_points = get_random_points(N_train)\n",
    "    m, b, t_function = get_target_function()\n",
    "    while len(np.unique(training_values)) == 1:\n",
    "        training_points = get_random_points(N_train)\n",
    "        m, b, t_function = get_target_function()\n",
    "        training_values = get_point_values(training_points,\n",
    "                                           t_function)\n",
    "    \n",
    "    # Initialize and fit PLA to training data.\n",
    "    pla = PLA()\n",
    "    pla.fit(training_points, training_values)\n",
    "    \n",
    "    # Initialize and fit SVM (Scikit-Learn LinearSVC) to training\n",
    "    # data\n",
    "    svc = SVC(C=float(\"inf\"), kernel=\"linear\",\n",
    "              decision_function_shape=\"ovr\")\n",
    "    # [:,1:] takes out the x0 = 1 artificial values\n",
    "    svc.fit(training_points[:,1:], training_values) \n",
    "    \n",
    "    # Initialize out-of-sample data\n",
    "    N_out_sample = 1000\n",
    "    out_sample_points = get_random_points(N_out_sample)\n",
    "    out_sample_values = get_point_values(out_sample_points,\n",
    "                                         t_function)\n",
    "    \n",
    "    # Compute the out-of-sample classification error of the PLA.\n",
    "    pla_out_sample_error = \\\n",
    "        pla.get_classification_error(out_sample_points,\n",
    "                                     out_sample_values)\n",
    "    \n",
    "    # Compute the out-of-sample classification error of the SVM.\n",
    "    # [:,1:] takes out the x0 = 1 artificial values\n",
    "    svc_out_sample_error = 1.0 - \\\n",
    "        svc.score(out_sample_points[:,1:], out_sample_values)\n",
    "    \n",
    "    return pla_out_sample_error, svc_out_sample_error, \\\n",
    "           len(svc.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLA_and_SVC_test(N_train, trials=1000):\n",
    "    \"\"\"\n",
    "    Return the average (using the given number of trials)\n",
    "    out-of-sample errors of the PLA (Perceptron Learning Algorithm)\n",
    "    and SVM (Support Vector Machine) and number of support\n",
    "    vectors of the SVM that resulted after being trained on randomly\n",
    "    generated datasets each of size N_train.\n",
    "    \"\"\"\n",
    "    total_svc_accurate_over_pla = 0\n",
    "    total_svc_n_support_vectors = 0\n",
    "    \n",
    "    for i in range(trials):\n",
    "        pla_out_sample_error, svc_out_sample_error, \\\n",
    "        svc_n_support_vectors = PLA_and_SVC_results(N_train)\n",
    "        \n",
    "        total_svc_accurate_over_pla += \\\n",
    "            int(svc_out_sample_error < pla_out_sample_error)\n",
    "        total_svc_n_support_vectors += svc_n_support_vectors\n",
    "    \n",
    "    return total_svc_accurate_over_pla / trials, \\\n",
    "           float(total_svc_n_support_vectors) / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Problem 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of time g_SVM is better than g_PLA in approximating f for N = 10: 62.1%\n"
     ]
    }
   ],
   "source": [
    "mean_svc_accurate_over_pla, mean_svc_n_support_vectors = \\\n",
    "    PLA_and_SVC_test(N_train=10)\n",
    "\n",
    "print(\"Percentage of time g_SVM is better than g_PLA in\",\n",
    "      \"approximating f for N = 10:\",\n",
    "      str(100 * mean_svc_accurate_over_pla) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Problems 9 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of time g_SVM is better than g_PLA in approximating f for N = 100: 68.8%\n",
      "Average number of support vectors of g_SVM for N = 100: 2.995\n"
     ]
    }
   ],
   "source": [
    "mean_svc_accurate_over_pla, mean_svc_n_support_vectors = \\\n",
    "    PLA_and_SVC_test(N_train=100)\n",
    "\n",
    "print(\"Percentage of time g_SVM is better than g_PLA in\",\n",
    "      \"approximating f for N = 100:\",\n",
    "      str(100 * mean_svc_accurate_over_pla) + \"%\")\n",
    "print(\"Average number of support vectors of g_SVM for N = 100:\",\n",
    "      mean_svc_n_support_vectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
