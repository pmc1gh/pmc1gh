CS 179
Assignment 2
Philip Carr

Due: Wednesday, April 17, 2019 - 3:00 PM.

Put all answers in a file called README.txt. After answering all of the
questions, list how long part 1 and part 2 took. Feel free to leave any other
feedback.

========================================
NOTE: New submission method!

Instead of emailing us the solution, put a zip file in your home directory 
on Titan, in the format:

lab2_2019_submission.zip


Your submission should be a single archive file (.zip) 
with your README file and all code.

========================================


PART 1

Question 1.1: Latency Hiding (5 points)
---------------------------------------

Approximately how many arithmetic instructions does it take to hide the latency
of a single arithmetic instruction on a GK110?

Assume all of the arithmetic instructions are independent (ie have no
instruction dependencies).

You do not need to consider the number of execution cores on the chip.

Hint: What is the latency of an arithmetic instruction? How many instructions
can a GK110 begin issuing in 1 clock cycle (assuming no dependencies)?

From Lecture 4, on a Kepler GPU (GK110), an arithmetic instruction has a latency
of 10ns. From Lecture 5, In 1 clock cycle, a GK110 can begin issuing instructions
to 4 warps and can issue 2 independent instructions per warp, for a total of 8
independent instructions issued per clock cycle. Since 1 clock cycle lasts 1ns
on a GK110, 10ns/(1ns / clock) * (8 arithmetic instructions / clock) = 80
arthmetic instructions can be issued during the time of latency of the original
arthmetic instruction. Thus, it takes 80 + 1 = 81 arthemtic instructions to hide
the latency of a single arithmetic instruction on a GK110 (this includes the
original arithmetic instruction as well).


Question 1.2: Thread Divergence (6 points)
------------------------------------------

Let the block shape be (32, 32, 1).

(a)
int idx = threadIdx.y + blockSize.y * threadIdx.x;
if (idx % 32 < 16)
    foo();
else
    bar();

Does this code diverge? Why or why not?

The block size has x-dimension of 32, so there are 32 threads per row of this
block. This allows warps to be arranged such that 1 warp exactly fits one row.
Since blockSize.y = 32, blockSize.y * threadIdx.x is always a multiple of 32,
so for threadIdx.y < 16 all corresponding rows/warps run foo(), while for
threadIdx.y >= 16, all corresponding rows/warps run bar(). Thus, this code
does not diverge.


(b)
const float pi = 3.14;
float result = 1.0;
for (int i = 0; i < threadIdx.x; i++)
    result *= pi;

Does this code diverge? Why or why not? (This is a bit of a trick question,
either "yes" or "no can be a correct answer with appropriate explanation.)

For each thread of some distinct threadIdx.x, the for loop runs threadIdx.x
times, which is different for different threads. Since threads in the same
warp still have different threadIdx.x values (assuming blockDim.x > 1), this
code diverges.


Question 1.3: Coalesced Memory Access (9 points)
------------------------------------------------

Let the block shape be (32, 32, 1). Let data be a (float *) pointing to global
memory and let data be 128 byte aligned (so data % 128 == 0).

Consider each of the following access patterns.

(a)
data[threadIdx.x + blockSize.x * threadIdx.y] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

This write is coalesced because consecutive memory accesses by threadIdx.x
occur consecutively in data since for some threadIdx.y and for threadIdx.x
and the next threadIdx.x value, (threadIdx.x + 1),
((threadIdx.x + 1) + blockSize.x * threadIdx.y)
- (threadIdx.x + blockSize.x * threadIdx.y) = 1. Since the block
shape x-dimension size is 32, warps exactly fit into each row of the block.
Thus, threadIdx.y is constant per warp, so arbitrarily set threadIdx.y = 0.
Since the minimum value of threadIdx.x + blockSize.x * threadIdx.y is
0 + 32 * 0 = 0
which corresponds to byte 0 * 4 = 0 in global memory pointed to by data
(since data is a float array and floats are 4 bytes each in size) and since
the maximum value of threadIdx.x + blockSize.x * threadIdx.y is
31 + 32 * 0 = 31 which corresponds to byte 31 * 4 = 124 in global memory
pointed to by data, this access pattern writes to floor(124 / 128) + 1 = 1
128 byte cache line.

(b)
data[threadIdx.y + blockSize.y * threadIdx.x] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

This write is not coalesced because consecutive memory accesses by
threadIdx.x do not occur consecutively in data since for some threadIdx.y and
for threadIdx.x and the next threadIdx.x value, (threadIdx.x + 1),
(threadIdx.y + blockSize.y * (threadIdx.x + 1))
- (threadIdx.y + blockSize.y * threadIdx.x) = blockSize.y. Since the block
shape x-dimension size is 32, warps exactly fit into each row of the block.
Thus, threadIdx.y is constant per warp, so arbitrarily set threadIdx.y = 0.
Since the minimum value of threadIdx.x + blockSize.x * threadIdx.y is
0 + 32 * 0 = 0
which corresponds to byte 0 * 4 = 0 in global memory pointed to by data
(since data is a float array and floats are 4 bytes each in size) and since
the maximum value of threadIdx.x + blockSize.x * threadIdx.y is
0 + 32 * 31 = 992 which corresponds to byte 992 * 4 = 3968 in global memory
pointed to by data, this access pattern writes to floor(3968 / 128) + 1 = 32
128 byte cache lines.

(c)
data[1 + threadIdx.x + blockSize.x * threadIdx.y] = 1.0;

Is this write coalesced? How many 128 byte cache lines does this write to?

This write is coalesced because consecutive memory accesses by threadIdx.x
occur consecutively in data since for some threadIdx.y and for threadIdx.x
and the next threadIdx.x value, (threadIdx.x + 1),
(1 + (threadIdx.x + 1) + blockSize.x * threadIdx.y)
- (1 + threadIdx.x + blockSize.x * threadIdx.y) = 1. Since the block
shape x-dimension size is 32, warps exactly fit into each row of the block.
Thus, threadIdx.y is constant per warp, so arbitrarily set threadIdx.y = 0.
Since the minimum value of 1 + threadIdx.x + blockSize.x * threadIdx.y is
1 + 0 + 32 * 0 = 1
which corresponds to byte 1 * 4 = 4 in global memory pointed to by data
(since data is a float array and floats are 4 bytes each in size) and since
the maximum value of 1 + threadIdx.x + blockSize.x * threadIdx.y is
1 + 31 + 32 * 0 = 32 which corresponds to byte 32 * 4 = 128 in global memory
pointed to by data, this access pattern writes to floor(128 / 128) + 1 = 2
128 byte cache lines.


Question 1.4: Bank Conflicts and Instruction Dependencies (15 points)
---------------------------------------------------------------------

Let's consider multiplying a 32 x 128 matrix with a 128 x 32 element matrix.
This outputs a 32 x 32 matrix. We'll use 32 ** 2 = 1024 threads and each thread
will compute 1 output element. Although its not optimal, for the sake of
simplicity let's use a single block, so grid shape = (1, 1, 1),
block shape = (32, 32, 1).

For the sake of this problem, let's assume both the left and right matrices have
already been stored in shared memory are in column major format. This means the
element in the ith row and jth column is accessible at lhs[i + 32 * j] for the
left hand side and rhs[i + 128 * j] for the right hand side.

This kernel will write to a variable called output stored in shared memory.

Consider the following kernel code:

int i = threadIdx.x;
int j = threadIdx.y;
for (int k = 0; k < 128; k += 2) {
    output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
    output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];
}

(a)
Are there bank conflicts in this code? If so, how many ways is the bank conflict
(2-way, 4-way, etc)?

Since there are 32 banks per block, bank accesses correspond to the array
index % 32. For accessing the output array, the index % 32 is
(i + 32 * j) % 32 = i % 32. Since there are sequential accesses of i, where
i increments by 1, there is a stride 1 access pattern for output, so no bank
conflicts occur here. For both lhs accesses above, they also have a stride 1
access pattern, since (i + 32 * k) % 32 = i % 32 and
(i + 32 * (k + 1)) % 32 = i % 32. However, for the rhs memory accesses, for a
given i and j, (k + 128 * j) % 32 = k % 32 and
((k + 1) + 128 * j) % 32 = (k + 1) % 32, so bank accesses are determined by k
here. Since for some given i and j, k loops from 0 to 127 inclusive, the shared
memory accesses of rhs occur in all 32 banks per given i and j, resulting in a
32-way bank conflict for each value of i = threadIdx.x and j = threadIdx.y.

(b)
Expand the inner part of the loop (below)

output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];
output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];

into "psuedo-assembly" as was done in the coordinate addition example in lecture
4.

There's no need to expand the indexing math, only to expand the loads, stores,
and math. Notably, the operation a += b * c can be computed by a single
instruction called a fused multiply add (FMA), so this can be a single
instruction in your "psuedo-assembly".

Hint: Each line should expand to 5 instructions.

"Psuedo-assembly" for output[i + 32 * j] += lhs[i + 32 * k] * rhs[k + 128 * j];:
1. lhs_k = lhs[i + 32 * k];
2. rhs_k = rhs[k + 128 * j];
3. outp_ij = output[i + 32 * j];
4. outp_ij += lhs_k * rhs_k;
5. output[i + 32 * j] = outp_ij;

"Psuedo-assembly" for
output[i + 32 * j] += lhs[i + 32 * (k + 1)] * rhs[(k + 1) + 128 * j];:
6. lhs_kp1 = lhs[i + 32 * (k + 1)];
7. rhs_kp1 = rhs[(k + 1) + 128 * j];
8. out_ij = output[i + 32 * j];
9. out_ij += lhs_kp1 * rhs_kp1;
10. output[i + 32 * j] = out_ij;

(c)
Identify pairs of dependent instructions in your answer to part b.

Pairs of dependent instructions in the "pseudo-assembly" in part b are 1 and 4,
2 and 4, 4 and 5, 6 and 9, 7 and 9, 9 and 10, 3 and 10 (because output[i + 32 * j]
in line 3 depends on line 10 where this array value is modified as the loop
progresses), and 5 and 8 (because output[i + 32 * j] in line 8 depends on line 5
where this array value is modified).

(d)
Rewrite the code given at the beginning of this problem to minimize instruction
dependencies. You can add or delete instructions (deleting an instruction is a
valid way to get rid of a dependency!) but each iteration of the loop must still
process 2 values of k.

int i = threadIdx.x;
int j = threadIdx.y;
for (int k = 0; k < 128; k += 2) {
    lhs_k = lhs[i + 32 * k];
    rhs_k = rhs[k + 128 * j];
    lhs_kp1 = lhs[i + 32 * (k + 1)];
    rhs_kp1 = rhs[(k + 1) + 128 * j];
    
    output[i + 32 * j] += lhs_k * rhs_k;
    output[i + 32 * j] += lhs_kp1 * rhs_kp1;
    (ONE LINE OR TWO FOR ADDITION for output array???)
}

(e)
Can you think of any other anything else you can do that might make this code
run faster?

Instead of accessing the output array twice during every iteration of the for
loop from k = 0 to 127 inclusive, initialize a local variable (register value)
output to equal output[i + 32 * j] before the for loop begins, then add
successive values of lhs_k * rhs_k and lhs_kp1 * rhs_kp1 (see part d) to this
new output variable within the for loop, then after the for loop ends, setting
output[i + 32 * j] equal to this output variable. This change would make the
code run faster by vastly decreasing the number of accesses to shared memory
where the output array is stored, instead accessing local (register) memory,
which is much faster to access than shared memory is.

Another way in which this code could be made to be run faster would be to unroll
this for loop more would be to have k increment with a larger value than 2, such
4, 8, 16, etc. (depending on the number of available registers that a thread has
access to) and do the respective matrix multiplication computation accessing
those additional variables per loop. This change would make the code run faster
because unrolling the for loop more increases the instruction level parallelism
(ILP) of the code.

Maximizing the number of threads in the GPU to utilize for this computation would
also make this code run faster, and using the parallel sum reduction could be
used if there are an arbitrary number of threads (where a linear time to log(n)
time speedup would occur).


PART 2 - Matrix transpose optimization (65 points)
--------------------------------------------------

Optimize the CUDA matrix transpose implementations in transpose_cuda.cu. Read
ALL of the TODO comments. Matrix transpose is a common exercise in GPU
optimization, so do not search for existing GPU matrix transpose code on the
internet.

Your transpose code only need to be able to transpose square matrices where the
side length is a multiple of 64.

The initial implementation has each block of 1024 threads handle a 64x64 block
of the matrix, but you can change anything about the kernel if it helps obtain
better performance.

The main method of transpose.cc already checks for correctness for all transpose
results, so there should be an assertion failure if your kernel produces incorrect
output.

The purpose of the shmemTransposeKernel is to demonstrate proper usage of global
and shared memory. The optimalTransposeKernel should be built on top of
shmemTransposeKernel and should incorporate any "tricks" such as ILP, loop
unrolling, vectorized IO, etc that have been discussed in class.

You can compile and run the code by running

make transpose
./transpose

and the build process was tested on minuteman. If this does not work on haru for
you, be sure to add the lines

export PATH=/usr/local/cuda-6.5/bin:$PATH
export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64:$LD_LIBRARY_PATH

to your ~/.profile file (and then exit and ssh back in to restart your shell).

On OS X, you may have to run or add to your .bash_profile the command

export DYLD_LIBRARY_PATH=$DYLD_LIBRARY_PATH:/usr/local/cuda/lib/

in order to get dynamic library linkage to work correctly.

The transpose program takes 2 optional arguments: input size and method. Input
size must be one of -1, 512, 1024, 2048, 4096, and method must be one all,
cpu, gpu_memcpy, naive, shmem, optimal. Input size is the first argument and
defaults to -1. Method is the second argument and defaults to all. You can pass
input size without passing method, but you cannot pass method without passing an
input size.

Examples:
./transpose
./transpose 512
./transpose 4096 naive
./transpose -1 optimal

Copy paste the output of ./transpose.cc into README.txt once you are done.
Describe the strategies used for performance in either block comments over the
kernel (as done for naiveTransposeKernel) or in README.txt.

output of ./transpose:
Index of the GPU with the lowest temperature: 1 (0 C)
Time limit for this program set to 10 seconds
Size 512 naive CPU: 0.906528 ms
Size 512 GPU memcpy: 0.025312 ms
Size 512 naive GPU: 0.044512 ms
Size 512 shmem GPU: 0.015712 ms
Size 512 optimal GPU: 0.022432 ms

Size 1024 naive CPU: 4.236992 ms
Size 1024 GPU memcpy: 0.055328 ms
Size 1024 naive GPU: 0.115232 ms
Size 1024 shmem GPU: 0.040992 ms
Size 1024 optimal GPU: 0.043904 ms

Size 2048 naive CPU: 39.378529 ms
Size 2048 GPU memcpy: 0.156480 ms
Size 2048 naive GPU: 0.389280 ms
Size 2048 shmem GPU: 0.139616 ms
Size 2048 optimal GPU: 0.138080 ms

Size 4096 naive CPU: 168.502533 ms
Size 4096 GPU memcpy: 0.540128 ms
Size 4096 naive GPU: 1.637056 ms
Size 4096 shmem GPU: 0.561120 ms
Size 4096 optimal GPU: 0.561088 ms

(See code for comments about strategies used for performance.)

BONUS (+5 points, maximum set score is 100 even with bonus)
--------------------------------------------------------------------------------

Mathematical scripting environments such as Matlab or Python + Numpy often
encourage expressing algorithms in terms of vector operations because they offer
a convenient and performant interface. For instance, one can add 2 n-component
vectors (a and b) in Numpy with c = a + b.

This is often implemented with something like the following code:

void vec_add(float *left, float *right, float *out, int size) {
    for (int i = 0; i < size; i++)
        out[i] = left[i] + right[i];
}

Consider the code

a = x + y + z

where x, y, z are n-component vectors.

One way this could be computed would be

vec_add(x, y, a, n);
vec_add(a, z, a, n);

In what ways is this code (2 calls to vec_add) worse than the following?

for (int i = 0; i < n; i++)
    a[i] = x[i] + y[i] + z[i];

List at least 2 ways (you don't need more than a sentence or two for each way).
