{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 155 Miniproject 1 (Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Phil\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'cross_val_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4f0209ea6240>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'cross_val_score'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, BatchNormalization\n",
    "from keras import regularizers\n",
    "\n",
    "# For the AUC metric\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.datasets import make_classification\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import Callback, EarlyStopping\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def load_data(filename, skiprows = 1):\n",
    "    \"\"\"\n",
    "    Function loads data stored in the file filename and returns it as a numpy ndarray.\n",
    "    \n",
    "    Inputs:\n",
    "        filename: given as a string.\n",
    "        \n",
    "    Outputs:\n",
    "        Data contained in the file, returned as a numpy ndarray\n",
    "    \"\"\"\n",
    "    return np.loadtxt(filename, skiprows=skiprows, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define roc_callback, inspired by https://github.com/keras-team/keras/issues/6050#issuecomment-329996505\n",
    "def auc_roc(y_true, y_pred):\n",
    "    # any tensorflow metric\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data and divide it into training and validation sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = load_data(\"train_2008.csv\")\n",
    "N = len(X)\n",
    "\n",
    "data = X[:, 3:-1]\n",
    "label = X[:, -1]\n",
    "\n",
    "train_percent = 70.\n",
    "train_size = int(N * train_percent / 100)\n",
    "\n",
    "random_order = np.random.permutation(np.arange(N))\n",
    "\n",
    "x_train = data[random_order[0:train_size]]\n",
    "y_train = label[random_order[0:train_size]]\n",
    "x_validation = data[random_order[train_size:]]\n",
    "y_validation = label[random_order[train_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(N)\n",
    "# print(train_size)\n",
    "print(np.shape(X))\n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # One-hot encode the labels.\n",
    "# y_train = keras.utils.np_utils.to_categorical(y_train)\n",
    "# y_validation = keras.utils.np_utils.to_categorical(y_validation)\n",
    "\n",
    "# class_weight = {0: np.sum(y_train[:,0]),\n",
    "#                 1: np.sum(y_train[:,1])}\n",
    "\n",
    "# print(class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to NORMALIZE\n",
    "train_mean_array = np.zeros(len(x_train[0]))\n",
    "train_std_array = np.zeros(len(x_train[0]))\n",
    "std_nonzero_indices = []\n",
    "for j in range(len(x_train[0])):\n",
    "    train_mean_array[j] = np.mean(x_train[:,j])\n",
    "    train_std_array[j] = np.std(x_train[:,j])\n",
    "    if train_std_array[j] != 0:\n",
    "        std_nonzero_indices.append(j)\n",
    "        x_train[:,j] = \\\n",
    "            np.divide(x_train[:,j] - train_mean_array[j],\n",
    "                      train_std_array[j])\n",
    "    if np.std(x_validation[:,j]) != 0:\n",
    "        x_validation[:,j] = \\\n",
    "            np.divide(x_validation[:,j] - np.mean(x_validation[:,j]),\n",
    "                      np.std(x_validation[:,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train[:, std_nonzero_indices]\n",
    "x_validation = x_validation[:, std_nonzero_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation/Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "rf = RandomForestRegressor(n_estimators=1000, n_jobs=-1, max_features=20,\n",
    "                           max_depth=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# our model has some # of parameters:\n",
    "# model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that our model outputs two eval params:\n",
    "# 1. loss (categorical cross-entropy)\n",
    "# 2. accuracy\n",
    "# model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_train = rf.predict(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the accuracy of the model, according to the loss function specified in model.compile above.\n",
    "train_score = rf.score(x_train, y_train)\n",
    "print('Train score:', train_score)\n",
    "train_auc = roc_auc_score(y_train, y_output_train)\n",
    "print('Train auc:', train_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_lines = []\n",
    "for i in range(len(y_output_train)):\n",
    "    y_output_lines.append([i, y_output_train[i], y_train[i]])\n",
    "np.savetxt(\"2008_train_output.csv\", y_output_lines, fmt='%d, %f, %f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Printing the accuracy of the model, according to the loss function specified in model.compile above.\n",
    "validation_score = rf.score(x_validation, y_validation)\n",
    "print('Validation score:', validation_score)\n",
    "y_output_validation = rf.predict(x_validation)\n",
    "validation_auc = roc_auc_score(y_validation, y_output_validation)\n",
    "print('Validation auc:', validation_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_lines = []\n",
    "for i in range(len(y_output_validation)):\n",
    "    y_output_lines.append([i, y_output_validation[i], y_validation[i]])\n",
    "np.savetxt(\"2008_validation_output.csv\", y_output_lines, fmt='%d, %f, %f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(cross_val_score(rf, data, label, cv=5, scoring=\"roc_auc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = load_data(\"test_2008.csv\")\n",
    "ids = X_test[:,0]\n",
    "\n",
    "x_test = X_test[:, 3:]\n",
    "# x_test = X[:, 3:-1]\n",
    "# y_test = X[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_test = keras.utils.np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't forget to NORMALIZE\n",
    "# std_nonzero_indices = []\n",
    "for j in range(len(x_test[0])):\n",
    "    test_std = np.std(x_test[:,j])\n",
    "    if test_std != 0:\n",
    "        # std_nonzero_indices.append(j)\n",
    "        x_test[:,j] = \\\n",
    "            np.divide(x_test[:,j] - np.mean(x_test[:,j]),\n",
    "                      np.std(x_test[:,j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test[:, std_nonzero_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ## Printing the accuracy of the model, according to the loss function specified in model.compile above.\n",
    "# test_score = model.evaluate(x=x_test, y=y_test, verbose=0)\n",
    "# print('Test score:', test_score[0])\n",
    "# print('Test accuracy:', test_score[1])\n",
    "# print('Test AUC:', test_score[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_output_lines2 = []\n",
    "for i in range(len(y_output)):\n",
    "    y_output_lines2.append([i, y_output[i]])\n",
    "np.savetxt(\"2008_submission.csv\", y_output_lines2, fmt='%d, %f')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
